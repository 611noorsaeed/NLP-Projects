{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d41b9d3",
   "metadata": {},
   "source": [
    "# NLP Poject No 1:\n",
    "# Named Entity Recognition (NER) NLP Projects\n",
    "\n",
    "Named Entity means anything that is a real-world object such as a person, a place, any organisation, any product which has a name. For example – “My name is Noor, and I and a Machine Learning Trainer”. In this sentence the name “Aman”, the field or subject “Machine Learning” and the profession “Trainer” are named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38464bd",
   "metadata": {},
   "source": [
    "In Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text.\n",
    "\n",
    "Have you ever used software known as Grammarly? It identifies all the incorrect spellings and punctuations in the text and corrects it. But it does not do anything with the named entities, as it is also using the same technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8da477",
   "metadata": {},
   "source": [
    "# Loading the Data for Named Entity Recognition (NER)\n",
    "The dataset, that I will use for this task can be easily downloaded from here. Now the first thing I will fo is to load the data and have a look at it to know what I am working with. So let’s simply import the pandas library and load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9d67b",
   "metadata": {},
   "source": [
    "In the data, we can see that the words are broken into columns which will represent our feature X, and the Tag column in the right will represent our label Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b79bf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898f544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ner_datasetreference.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046fef19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag\n",
       "0  Sentence: 1      Thousands  NNS      O\n",
       "1          NaN             of   IN      O\n",
       "2          NaN  demonstrators  NNS      O\n",
       "3          NaN           have  VBP      O\n",
       "4          NaN        marched  VBN      O\n",
       "5          NaN        through   IN      O\n",
       "6          NaN         London  NNP  B-geo\n",
       "7          NaN             to   TO      O\n",
       "8          NaN        protest   VB      O\n",
       "9          NaN            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30cb0d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51e4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thousands', 'of', 'demonstrators', ..., 'to', 'the', 'attack'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77164d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['O', 'O', 'O', ..., 'O', 'O', 'O'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tag'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75753e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #    1000616\n",
       "Word                0\n",
       "POS                 0\n",
       "Tag                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8971887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "953668"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728d5e2",
   "metadata": {},
   "source": [
    "# Data Preparation for Neural Networks\n",
    "I will train a Neural Network for the task of Named Entity Recognition (NER). So we need to do some modifications in the data to prepare it in such a manner so that it can easily fit into a neutral network. I will start this step by extracting the mappings that are required to train the neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a724234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_dict_map(df,token_or_tag):\n",
    "    token_to_index = {}\n",
    "    index_to_token = {}\n",
    "    \n",
    "    if token_or_tag == \"token\":\n",
    "        vocab = list(set(df['Word'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(df['Tag'].to_list()))\n",
    "        \n",
    "    token_to_index = {ktn:idx for ktn,idx in enumerate(vocab)}\n",
    "    index_to_token = {idx:ktn for ktn,idx in enumerate(vocab)}\n",
    "    \n",
    "    return token_to_index,index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aea6e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn_to_idx, idx_to_tkn = get_dict_map(df,'token')\n",
    "tag_to_idx, idx_to_tag = get_dict_map(df,'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "302f3a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-art',\n",
       " 1: 'I-org',\n",
       " 2: 'I-per',\n",
       " 3: 'I-nat',\n",
       " 4: 'B-nat',\n",
       " 5: 'B-tim',\n",
       " 6: 'B-eve',\n",
       " 7: 'B-per',\n",
       " 8: 'I-gpe',\n",
       " 9: 'I-tim',\n",
       " 10: 'B-geo',\n",
       " 11: 'B-org',\n",
       " 12: 'I-art',\n",
       " 13: 'I-eve',\n",
       " 14: 'I-geo',\n",
       " 15: 'O',\n",
       " 16: 'B-gpe'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439c036b",
   "metadata": {},
   "source": [
    "# Now I will transform the columns in the data to extract the sequential data for our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "912e3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_idx'] = df['Word'].map(tkn_to_idx)\n",
    "df['tag_idx'] = df['Tag'].map(tag_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92d3be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df.fillna(method='ffill',axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69125dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.fillna(-1,axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa812365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>word_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>Sentence: 47959</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Sentence #           Word  POS Tag  word_idx  tag_idx\n",
       "0            Sentence: 1      Thousands  NNS   O      -1.0     -1.0\n",
       "1            Sentence: 1             of   IN   O      -1.0     -1.0\n",
       "2            Sentence: 1  demonstrators  NNS   O      -1.0     -1.0\n",
       "3            Sentence: 1           have  VBP   O      -1.0     -1.0\n",
       "4            Sentence: 1        marched  VBN   O      -1.0     -1.0\n",
       "...                  ...            ...  ...  ..       ...      ...\n",
       "1048570  Sentence: 47959           they  PRP   O      -1.0     -1.0\n",
       "1048571  Sentence: 47959      responded  VBD   O      -1.0     -1.0\n",
       "1048572  Sentence: 47959             to   TO   O      -1.0     -1.0\n",
       "1048573  Sentence: 47959            the   DT   O      -1.0     -1.0\n",
       "1048574  Sentence: 47959         attack   NN   O      -1.0     -1.0\n",
       "\n",
       "[1048575 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850787d4",
   "metadata": {},
   "source": [
    "# Groupby and collect columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecced62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noor Saeed\\AppData\\Local\\Temp\\ipykernel_3832\\3816634151.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  data_group = df_filled.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'word_idx', 'tag_idx'].agg(lambda x: list(x))\n"
     ]
    }
   ],
   "source": [
    "# Groupby and collect columns\n",
    "data_group = df_filled.groupby(['Sentence #'],as_index=False)['Word', 'POS', 'Tag', 'word_idx', 'tag_idx'].agg(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d17f92",
   "metadata": {},
   "source": [
    "# split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8a60c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efaeb887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens length: 32372 \n",
      "train_tokens length: 32372 \n",
      "test_tokens length: 4796 \n",
      "test_tags: 4796 \n",
      "val_tokens: 10791 \n",
      "val_tags: 10791\n"
     ]
    }
   ],
   "source": [
    "def get_pad_train_test_val(data_group,df):\n",
    "    \n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(df['Word'].to_list())))\n",
    "    n_tag = len(list(set(df['Tag'].to_list())))\n",
    "    \n",
    "    #Pad tokens (X var)\n",
    "    tokens = data_group['word_idx'].fillna(-1).tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens,maxlen=maxlen,dtype='int32',padding='post',value=n_token - 1)\n",
    "    \n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['tag_idx'].fillna(-1).tolist()\n",
    "#     tag_to_idx['O'] = len(tag_to_idx)\n",
    "    pad_tags = pad_sequences(tags,maxlen=maxlen,dtype='int32',padding='post',value=tag_to_idx['O'])\n",
    "    n_tags = len(tag_to_idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2023)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2023)\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        'train_tokens length:', len(train_tokens),\n",
    "        '\\ntrain_tokens length:', len(train_tokens),\n",
    "        '\\ntest_tokens length:', len(test_tokens),\n",
    "        '\\ntest_tags:', len(test_tags),\n",
    "        '\\nval_tokens:', len(val_tokens),\n",
    "        '\\nval_tags:', len(val_tags),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(data_group, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfb4ce",
   "metadata": {},
   "source": [
    "# Training Neural Network for Named Entity Recognition (NER)\n",
    "Now, I will proceed with training the neural network architecture of our model. So let’s start with importing all the packages we need for training our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9528971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f40a6",
   "metadata": {},
   "source": [
    "# The layer below will take the dimensions from the LSTM layer and will give the maximum length and maximum tags as an output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a659d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(list(set(df['Word'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in data_group['word_idx'].tolist()])\n",
    "n_tags = len(tag_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed467a5",
   "metadata": {},
   "source": [
    "# Now I will create a helper function which will help us in giving the summary of every layer of the neural network model for Named Entity Recognition (NER):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3f3f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_lstm_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Embedding layer\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "\n",
    "    # Add bidirectional LSTM\n",
    "    model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "\n",
    "    # Add LSTM\n",
    "    model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "\n",
    "    # Add timeDistributed Layer\n",
    "    model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "\n",
    "    #Optimiser \n",
    "    # adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e26ed",
   "metadata": {},
   "source": [
    "# Now I will create a helper function to train the Named Entity Recognition model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0159230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    print(X.shape,'\\n',y.shape)\n",
    "    loss = list()\n",
    "    for i in range(25):\n",
    "        # fit model for one epoch on this sequence\n",
    "        hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1,shuffle=True, validation_split=0.2)\n",
    "        loss.append(hist.history['loss'][0])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350711d",
   "metadata": {},
   "source": [
    "# Driver code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f66b7a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 104, 64)           2251456   \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 104, 128)         66048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 104, 64)           49408     \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 104, 18)          1170      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,368,082\n",
      "Trainable params: 2,368,082\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n",
      "(32372, 104) \n",
      " (32372, 104, 18)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_5/embedding_5/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Temp\\ipykernel_3832\\329638798.py\", line 4, in <module>\n      results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Temp\\ipykernel_3832\\3288993200.py\", line 6, in train_model\n      hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1,shuffle=True, validation_split=0.2)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_5/embedding_5/embedding_lookup'\nindices[938,0] = -1 is not in [0, 35179)\n\t [[{{node sequential_5/embedding_5/embedding_lookup}}]] [Op:__inference_train_function_63793]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [63], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model_bilstm_lstm \u001b[38;5;241m=\u001b[39m get_bilstm_lstm_model()\n\u001b[0;32m      3\u001b[0m plot_model(model_bilstm_lstm)\n\u001b[1;32m----> 4\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith_add_lstm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_bilstm_lstm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [62], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, y, model)\u001b[0m\n\u001b[0;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m25\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# fit model for one epoch on this sequence\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_5/embedding_5/embedding_lookup' defined at (most recent call last):\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n      app.start()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Temp\\ipykernel_3832\\329638798.py\", line 4, in <module>\n      results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Temp\\ipykernel_3832\\3288993200.py\", line 6, in train_model\n      hist = model.fit(X, y, batch_size=1000, verbose=1, epochs=1,shuffle=True, validation_split=0.2)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\sequential.py\", line 410, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\Noor Saeed\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\core\\embedding.py\", line 208, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'sequential_5/embedding_5/embedding_lookup'\nindices[938,0] = -1 is not in [0, 35179)\n\t [[{{node sequential_5/embedding_5/embedding_lookup}}]] [Op:__inference_train_function_63793]"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame()\n",
    "model_bilstm_lstm = get_bilstm_lstm_model()\n",
    "plot_model(model_bilstm_lstm)\n",
    "results['with_add_lstm'] = train_model(train_tokens, np.array(train_tags), model_bilstm_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add24a6c",
   "metadata": {},
   "source": [
    "# Testing the Named Entity Recognition (NER) Model:\n",
    "Now let’s test our model on a piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81b46f2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m displacy\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp('Hi, My name is Noor Saeed \\n I am from Pakistan \\n I want to work with Google \\n Steve Jobs is My Inspiration')\n",
    "displacy.render(text, style = 'ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5339354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
